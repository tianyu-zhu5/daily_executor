#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æ¨é€è„šæœ¬ - æ¯æ—¥æ‰§è¡Œå™¨

åŠŸèƒ½ï¼š
1. æ›´æ–°Kçº¿æ•°æ®ï¼ˆè¿è¡Œ stock_data_manager.pyï¼‰
2. ç”Ÿæˆå½“å¤©ä¹°å…¥ä¿¡å·ï¼ˆè¿è¡Œ export_cci_signals_for_simulation.pyï¼‰
3. æ¨é€ä¿¡å·åˆ°å¾®ä¿¡ï¼ˆé€šè¿‡ wechat_pusher.pyï¼‰

æ‰§è¡Œæ—¶é—´ï¼šæ¯ä¸ªäº¤æ˜“æ—¥16:00ï¼ˆé€šè¿‡Windowsè®¡åˆ’ä»»åŠ¡ï¼‰

Author: Daily Executor System
Date: 2025-11-10
"""

import sys
import os
import json
import subprocess
import logging
import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import traceback

# æ·»åŠ CCI-Divergenceåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / 'CCI-Divergence'))

# Import local modules
from query_engine import QueryEngine
from signal_types import Signal

# æ·»åŠ UTF-8ç¼–ç æ”¯æŒï¼ˆWindowsï¼‰
if sys.platform == 'win32':
    try:
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8')
        if hasattr(sys.stderr, 'reconfigure'):
            sys.stderr.reconfigure(encoding='utf-8')
    except Exception as e:
        print(f"Warning: Could not reconfigure stdout/stderr encoding: {e}")


class DailyExecutor:
    """æ¯æ—¥è‡ªåŠ¨åŒ–æ‰§è¡Œå™¨"""

    def __init__(self, config_file: str = "config.json"):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = Path(config_file)
        self.script_dir = Path(__file__).parent.absolute()
        self.config = self._load_config()
        self.logger = self._setup_logging()

    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise

    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = Path(self.config['logging']['log_dir'])
        log_dir.mkdir(parents=True, exist_ok=True)

        log_level = getattr(logging, self.config['logging']['log_level'], logging.INFO)

        # æ—¥å¿—æ–‡ä»¶åï¼šexecutor_YYYYMMDD_HHMMSS.log
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = log_dir / f"executor_{timestamp}.log"

        # é…ç½®æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )

        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(log_level)
        file_handler.setFormatter(formatter)

        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(log_level)
        console_handler.setFormatter(formatter)

        # é…ç½®logger
        logger = logging.getLogger('DailyExecutor')
        logger.setLevel(log_level)
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

        logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ: {log_file}")

        return logger

    def _run_subprocess(
        self,
        command: list,
        cwd: Optional[Path] = None,
        timeout: Optional[int] = None,
        step_name: str = "æ‰§è¡Œå‘½ä»¤"
    ) -> bool:
        """
        è¿è¡Œå­è¿›ç¨‹

        Args:
            command: å‘½ä»¤åˆ—è¡¨
            cwd: å·¥ä½œç›®å½•
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            step_name: æ­¥éª¤åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        self.logger.info(f"[{step_name}] å¼€å§‹æ‰§è¡Œ")
        self.logger.info(f"[{step_name}] å‘½ä»¤: {' '.join(command)}")
        if cwd:
            self.logger.info(f"[{step_name}] å·¥ä½œç›®å½•: {cwd}")

        start_time = datetime.now()

        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿å­è¿›ç¨‹ä½¿ç”¨UTF-8ç¼–ç 
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'  # Python 3.7+

            # è¿è¡Œå­è¿›ç¨‹
            result = subprocess.run(
                command,
                cwd=cwd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=timeout,
                errors='replace',  # å¤„ç†ç¼–ç é”™è¯¯
                env=env  # ä¼ é€’ç¯å¢ƒå˜é‡
            )

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # è®°å½•è¾“å‡º
            if result.stdout:
                self.logger.info(f"[{step_name}] æ ‡å‡†è¾“å‡º:\n{result.stdout}")

            if result.stderr:
                if result.returncode == 0:
                    self.logger.warning(f"[{step_name}] æ ‡å‡†é”™è¯¯:\n{result.stderr}")
                else:
                    self.logger.error(f"[{step_name}] æ ‡å‡†é”™è¯¯:\n{result.stderr}")

            # æ£€æŸ¥è¿”å›ç 
            if result.returncode == 0:
                self.logger.info(f"[{step_name}] æ‰§è¡ŒæˆåŠŸ (è€—æ—¶: {duration:.2f}ç§’)")
                return True
            else:
                self.logger.error(f"[{step_name}] æ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})")
                return False

        except subprocess.TimeoutExpired:
            self.logger.error(f"[{step_name}] æ‰§è¡Œè¶…æ—¶ (è¶…è¿‡ {timeout}ç§’)")
            return False
        except Exception as e:
            self.logger.error(f"[{step_name}] æ‰§è¡Œå¼‚å¸¸: {e}")
            self.logger.error(traceback.format_exc())
            return False

    def _get_today_date(self, custom_date: str = None) -> str:
        """
        è·å–ä»Šå¤©çš„æ—¥æœŸå­—ç¬¦ä¸²

        Args:
            custom_date: è‡ªå®šä¹‰æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä»Šå¤©

        Returns:
            YYYY-MM-DD æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
        """
        if custom_date:
            # éªŒè¯æ—¥æœŸæ ¼å¼
            try:
                datetime.strptime(custom_date, '%Y-%m-%d')
                return custom_date
            except ValueError:
                self.logger.warning(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {custom_date}ï¼Œä½¿ç”¨ä»Šå¤©æ—¥æœŸ")
                return datetime.now().strftime('%Y-%m-%d')
        return datetime.now().strftime('%Y-%m-%d')

    def _read_stock_pool(self, stock_pool_file: str) -> Optional[str]:
        """
        è¯»å–è‚¡ç¥¨æ± æ–‡ä»¶

        Args:
            stock_pool_file: è‚¡ç¥¨æ± æ–‡ä»¶è·¯å¾„

        Returns:
            é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        stock_pool_path = Path(stock_pool_file)

        if not stock_pool_path.exists():
            self.logger.error(f"è‚¡ç¥¨æ± æ–‡ä»¶ä¸å­˜åœ¨: {stock_pool_path}")
            return None

        try:
            with open(stock_pool_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # è¿‡æ»¤ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
            stock_codes = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æå–è‚¡ç¥¨ä»£ç ï¼ˆå‡è®¾æ¯è¡Œä¸€ä¸ªä»£ç ï¼‰
                    stock_codes.append(line)

            if not stock_codes:
                self.logger.warning(f"è‚¡ç¥¨æ± æ–‡ä»¶ä¸ºç©º: {stock_pool_path}")
                return None

            stock_list = ','.join(stock_codes)
            self.logger.info(f"æˆåŠŸè¯»å–è‚¡ç¥¨æ± : {len(stock_codes)} åªè‚¡ç¥¨")

            return stock_list

        except Exception as e:
            self.logger.error(f"è¯»å–è‚¡ç¥¨æ± æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def step1_update_kline_data(self) -> bool:
        """
        æ­¥éª¤1: æ›´æ–°Kçº¿æ•°æ®

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤1: æ›´æ–°Kçº¿æ•°æ®")
        self.logger.info("=" * 80)

        script_path = Path(self.config['data_update']['script_path'])
        timeout = self.config['data_update']['timeout_seconds']

        # æ£€æŸ¥è„šæœ¬æ˜¯å¦å­˜åœ¨
        if not script_path.exists():
            self.logger.error(f"æ•°æ®æ›´æ–°è„šæœ¬ä¸å­˜åœ¨: {script_path}")
            return False

        # è¿è¡Œè„šæœ¬ï¼ˆä½¿ç”¨conda quantç¯å¢ƒï¼‰
        command = ['conda', 'run', '-n', 'quant', 'python', script_path.name]
        cwd = script_path.parent

        success = self._run_subprocess(
            command=command,
            cwd=cwd,
            timeout=timeout,
            step_name="æ›´æ–°Kçº¿æ•°æ®"
        )

        return success

    def step1_5_update_cci_divergence(self, custom_date: str = None) -> bool:
        """
        æ­¥éª¤1.5: æ›´æ–°CCIåº•èƒŒç¦»æ•°æ®

        Args:
            custom_date: è‡ªå®šä¹‰æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤1.5: æ›´æ–°CCIåº•èƒŒç¦»æ•°æ®")
        self.logger.info("=" * 80)

        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼ˆé¿å…å¯åŠ¨æ—¶åŠ è½½ï¼‰
            from src.signals.cci_generator import CCIDivergenceGenerator
            from src.database.cci_database import CCIDatabase
        except ImportError as e:
            self.logger.error(f"å¯¼å…¥CCIæ¨¡å—å¤±è´¥: {e}")
            self.logger.error("è¯·ç¡®ä¿ CCI-Divergence é¡¹ç›®åœ¨æ­£ç¡®ä½ç½®")
            return False

        config = self.config['cci_update']
        signal_config = self.config['signal_generation']

        # è·å–æ—¥æœŸ
        target_date = self._get_today_date(custom_date)
        self.logger.info(f"ç›®æ ‡æ—¥æœŸ: {target_date}")

        # è¯»å–è‚¡ç¥¨æ± 
        stock_pool_file = signal_config.get('stock_pool_file')
        if not stock_pool_file:
            self.logger.warning("æœªé…ç½®è‚¡ç¥¨æ± ï¼Œå°†å¤„ç†æ‰€æœ‰è‚¡ç¥¨")
            stock_codes = None
        else:
            stock_list_str = self._read_stock_pool(stock_pool_file)
            if stock_list_str is None:
                self.logger.error("è¯»å–è‚¡ç¥¨æ± å¤±è´¥")
                return False
            stock_codes = stock_list_str.split(',')
            self.logger.info(f"è‚¡ç¥¨æ± : {len(stock_codes)} åªè‚¡ç¥¨")

        # åˆå§‹åŒ–ç”Ÿæˆå™¨å’Œæ•°æ®åº“
        generator = CCIDivergenceGenerator(
            cci_period=config['cci_period'],
            pivot_window=config['pivot_window'],
            divergence_validity_days=config['divergence_validity_days']
        )

        db_path = Path(config['local_db_path'])
        db_path.parent.mkdir(parents=True, exist_ok=True)

        # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå…ˆåˆå§‹åŒ–
        if not db_path.exists() or db_path.stat().st_size == 0:
            self.logger.info(f"åˆå§‹åŒ–æœ¬åœ°CCIæ•°æ®åº“: {db_path}")

        data_dir = Path(config['data_dir'])

        # ç»Ÿè®¡
        total_processed = 0
        total_divergences = 0
        success_count = 0
        error_count = 0

        # å¤„ç†è‚¡ç¥¨æ± ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        stocks_to_process = stock_codes if stock_codes else []

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè‚¡ç¥¨æ± ï¼Œè·å–æ‰€æœ‰CSVæ–‡ä»¶
        if not stocks_to_process:
            csv_files = list(data_dir.glob("*.csv"))
            stocks_to_process = [f.stem for f in csv_files]
            self.logger.info(f"æœªæŒ‡å®šè‚¡ç¥¨æ± ï¼Œå°†å¤„ç† {len(stocks_to_process)} åªè‚¡ç¥¨")

        self.logger.info(f"å¼€å§‹å¤„ç† {len(stocks_to_process)} åªè‚¡ç¥¨...")

        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨æäº¤äº‹åŠ¡
        with CCIDatabase(str(db_path)) as db:
            # ç¡®ä¿æ•°æ®åº“è¡¨å·²åˆ›å»º
            db.create_tables()
            for stock_code in stocks_to_process:
                total_processed += 1

                try:
                    # è¯»å–Kçº¿æ•°æ®
                    csv_path = data_dir / f"{stock_code}.csv"
                    if not csv_path.exists():
                        self.logger.debug(f"{stock_code}: CSVæ–‡ä»¶ä¸å­˜åœ¨")
                        continue

                    df = pd.read_csv(csv_path)

                    if 'date' not in df.columns:
                        self.logger.warning(f"{stock_code}: ç¼ºå°‘dateåˆ—")
                        continue

                    if len(df) == 0:
                        self.logger.debug(f"{stock_code}: æ•°æ®ä¸ºç©º")
                        continue

                    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD
                    try:
                        # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åæ£€æµ‹æ ¼å¼
                        df['date'] = df['date'].astype(str).str.strip()

                        # æ£€æµ‹ç¬¬ä¸€ä¸ªéç©ºæ—¥æœŸçš„æ ¼å¼
                        first_date = df['date'].iloc[0]

                        # å¦‚æœæ˜¯çº¯æ•°å­—æ ¼å¼ï¼ˆYYYYMMDDï¼‰ï¼Œè½¬æ¢ä¸º YYYY-MM-DD
                        if first_date.replace('.', '').replace('-', '').isdigit() and len(first_date.replace('.', '').replace('-', '')) == 8:
                            if '-' not in first_date and '.' not in first_date:
                                # æ ¼å¼ï¼š20251108 -> 2025-11-08
                                df['date'] = pd.to_datetime(df['date'], format='%Y%m%d').dt.strftime('%Y-%m-%d')
                            elif '.' in first_date:
                                # æ ¼å¼ï¼š2025.11.08 -> 2025-11-08
                                df['date'] = df['date'].str.replace('.', '-')
                    except Exception as date_err:
                        self.logger.error(f"{stock_code}: æ—¥æœŸæ ¼å¼è½¬æ¢å¤±è´¥ - {type(date_err).__name__}: {date_err}")
                        continue

                    # è¿‡æ»¤åˆ°ç›®æ ‡æ—¥æœŸï¼ˆåŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®ç”¨äºCCIè®¡ç®—ï¼‰
                    # éœ€è¦è‡³å°‘ cci_period + pivot_window å¤©çš„æ•°æ®
                    min_required_days = config['cci_period'] + config['pivot_window'] + 10

                    # æ‰¾åˆ°ç›®æ ‡æ—¥æœŸçš„ä½ç½®
                    df_filtered = df[df['date'] <= target_date].tail(min_required_days + 50)

                    if len(df_filtered) < 40:
                        self.logger.debug(f"{stock_code}: æ•°æ®ä¸è¶³ ({len(df_filtered)}è¡Œ)")
                        continue

                    # é‡è¦ï¼šé‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ç´¢å¼•ä»0å¼€å§‹è¿ç»­
                    # divergence_detector ä¾èµ–è¿ç»­çš„æ•´æ•°ç´¢å¼•
                    df_filtered = df_filtered.reset_index(drop=True)

                    # æ£€æµ‹åº•èƒŒç¦»
                    divergences_df = generator.detect_divergences(df_filtered, stock_code)

                    # åªä¿å­˜ç›®æ ‡æ—¥æœŸçš„èƒŒç¦»
                    if len(divergences_df) > 0:
                        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
                        required_cols = ['divergence_id', 'stock_code', 'start_date', 'end_date',
                                       'start_price', 'end_price', 'start_cci', 'end_cci',
                                       'confidence', 'days_between', 'validity_days', 'expiry_date', 'status']

                        missing_cols = [col for col in required_cols if col not in divergences_df.columns]
                        if missing_cols:
                            self.logger.error(f"{stock_code}: åº•èƒŒç¦»DataFrameç¼ºå°‘åˆ—: {missing_cols}")
                            self.logger.error(f"{stock_code}: å®é™…åˆ—: {list(divergences_df.columns)}")
                            continue

                        # è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼ï¼ˆend_dateå¯èƒ½æ˜¯datetimeæˆ–å¸¦æ—¶é—´æˆ³çš„å­—ç¬¦ä¸²ï¼‰
                        divergences_df['end_date'] = pd.to_datetime(divergences_df['end_date']).dt.strftime('%Y-%m-%d')
                        divergences_df['start_date'] = pd.to_datetime(divergences_df['start_date']).dt.strftime('%Y-%m-%d')
                        divergences_df['expiry_date'] = pd.to_datetime(divergences_df['expiry_date']).dt.strftime('%Y-%m-%d')

                    if len(divergences_df) == 0:
                        continue

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    new_divergences = 0
                    duplicate_divergences = 0
                    for _, div in divergences_df.iterrows():
                        divergence_dict = {
                            'divergence_id': div['divergence_id'],
                            'stock_code': div['stock_code'],
                            'start_date': div['start_date'],
                            'end_date': div['end_date'],
                            'start_price': float(div['start_price']),
                            'end_price': float(div['end_price']),
                            'start_cci': float(div['start_cci']),
                            'end_cci': float(div['end_cci']),
                            'confidence': float(div['confidence']),
                            'days_between': int(div['days_between']),
                            'validity_days': int(div['validity_days']),
                            'expiry_date': div['expiry_date'],
                            'status': div['status']
                        }
                        try:
                            db.insert_divergence(divergence_dict)
                            new_divergences += 1
                        except Exception as e:
                            if 'UNIQUE constraint failed' in str(e):
                                duplicate_divergences += 1
                                self.logger.debug(f"{stock_code}: èƒŒç¦» {div['divergence_id']} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            else:
                                raise

                    total_divergences += new_divergences
                    success_count += 1

                    if len(divergences_df) > 0:
                        if duplicate_divergences > 0:
                            self.logger.info(f"{stock_code}: å‘ç° {len(divergences_df)} ä¸ªåº•èƒŒç¦» (æ–°å¢{new_divergences}ä¸ªï¼Œé‡å¤{duplicate_divergences}ä¸ª)")
                        else:
                            self.logger.info(f"{stock_code}: å‘ç° {len(divergences_df)} ä¸ªåº•èƒŒç¦»")

                except Exception as e:
                    error_count += 1
                    error_msg = str(e) if str(e) else repr(e)
                    self.logger.error(f"{stock_code}: å¤„ç†å¤±è´¥ - {type(e).__name__}: {error_msg}")
                    # æ‰“å°å®Œæ•´å †æ ˆï¼ˆå³ä½¿åœ¨INFOçº§åˆ«ä¹Ÿæ˜¾ç¤ºï¼‰
                    import traceback as tb
                    self.logger.error(f"{stock_code}: å®Œæ•´é”™è¯¯:\n{tb.format_exc()}")

        # ç»Ÿè®¡ç»“æœ
        self.logger.info("")
        self.logger.info("=" * 80)
        self.logger.info("CCIåº•èƒŒç¦»æ›´æ–°å®Œæˆ")
        self.logger.info("=" * 80)
        self.logger.info(f"å¤„ç†è‚¡ç¥¨æ•°: {total_processed}")
        self.logger.info(f"æˆåŠŸè‚¡ç¥¨æ•°: {success_count}")
        self.logger.info(f"é”™è¯¯è‚¡ç¥¨æ•°: {error_count}")
        self.logger.info(f"æ€»èƒŒç¦»æ•°: {total_divergences}")
        self.logger.info(f"æ•°æ®åº“è·¯å¾„: {db_path.absolute()}")
        self.logger.info("=" * 80)

        return True

    def step2_generate_signals(self, custom_date: str = None) -> bool:
        """
        æ­¥éª¤2: ç”Ÿæˆä¹°å…¥ä¿¡å·

        Uses QueryEngine to fetch signals from database instead of calling
        external script. This improves code reuse and maintainability.

        Args:
            custom_date: è‡ªå®šä¹‰æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤2: ç”Ÿæˆä¹°å…¥ä¿¡å·")
        self.logger.info("=" * 80)

        config = self.config['signal_generation']

        # è·å–æ—¥æœŸï¼ˆè‡ªå®šä¹‰æˆ–ä»Šå¤©ï¼‰
        today = self._get_today_date(custom_date)
        self.logger.info(f"ç›®æ ‡æ—¥æœŸ: {today}")

        try:
            # è¯»å–è‚¡ç¥¨æ± 
            stock_codes = None
            if config.get('stock_pool_file'):
                stock_list_str = self._read_stock_pool(config['stock_pool_file'])
                if stock_list_str:
                    stock_codes = stock_list_str.split(',')
                    self.logger.info(f"è‚¡ç¥¨æ± : {len(stock_codes)} åªè‚¡ç¥¨")
                else:
                    self.logger.warning("æœªæŒ‡å®šè‚¡ç¥¨æ± ï¼Œå°†ç”Ÿæˆæ‰€æœ‰è‚¡ç¥¨çš„ä¿¡å·")

            # Initialize QueryEngine
            query_engine = QueryEngine(
                db_path=config['db_path'],
                data_dir=config['data_dir']
            )

            # Fetch signals using QueryEngine
            signals = query_engine.get_signals_for_date(
                signal_date=today,
                stock_codes=stock_codes,
                min_confidence=config['min_confidence'],
                use_next_day_open=config.get('use_next_day_open', True)
            )

            if len(signals) == 0:
                self.logger.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¿¡å·")
            else:
                self.logger.info(f"ç”Ÿæˆ {len(signals)} ä¸ªä¹°å…¥ä¿¡å·")

            # Save signals to CSV
            output_file = Path(config['output_file'])
            output_file.parent.mkdir(parents=True, exist_ok=True)

            # Convert signals to DataFrame
            signals_data = [signal.to_dict() for signal in signals]
            signals_df = pd.DataFrame(signals_data)

            # Save to CSV
            signals_df.to_csv(output_file, index=False, encoding='utf-8-sig')

            self.logger.info(f"ä¿¡å·å·²ä¿å­˜åˆ°: {output_file}")

            return True

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆä¿¡å·å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            return False

    def step3_push_to_wechat(self) -> bool:
        """
        æ­¥éª¤3: æ¨é€åˆ°å¾®ä¿¡

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤3: æ¨é€åˆ°å¾®ä¿¡")
        self.logger.info("=" * 80)

        # æ£€æŸ¥ä¿¡å·æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        signals_file = Path(self.config['signal_generation']['output_file'])

        if not signals_file.exists():
            self.logger.error(f"ä¿¡å·æ–‡ä»¶ä¸å­˜åœ¨: {signals_file}")
            return False

        # å¯¼å…¥æ¨é€æ¨¡å—
        try:
            from wechat_pusher import WechatPusher

            pusher = WechatPusher(config_file=str(self.config_file))
            success = pusher.push_signals(str(signals_file))

            return success

        except Exception as e:
            self.logger.error(f"æ¨é€å¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            return False

    def execute(self, skip_steps: list = None, custom_date: str = None) -> bool:
        """
        æ‰§è¡Œå®Œæ•´æµç¨‹

        Args:
            skip_steps: è¦è·³è¿‡çš„æ­¥éª¤åˆ—è¡¨ï¼Œä¾‹å¦‚ ['step1', 'step3']
            custom_date: è‡ªå®šä¹‰æ—¥æœŸ (YYYY-MM-DD æ ¼å¼)ï¼Œç”¨äºæµ‹è¯•å†å²æ—¥æœŸ

        Returns:
            æ˜¯å¦å…¨éƒ¨æˆåŠŸ
        """
        skip_steps = skip_steps or []

        self.logger.info("=" * 80)
        self.logger.info("è‡ªåŠ¨åŒ–æ¨é€è„šæœ¬å¼€å§‹æ‰§è¡Œ")
        self.logger.info(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if custom_date:
            self.logger.info(f"æµ‹è¯•æ—¥æœŸ: {custom_date}")
        if skip_steps:
            self.logger.info(f"è·³è¿‡æ­¥éª¤: {', '.join(skip_steps)}")
        self.logger.info("=" * 80)

        start_time = datetime.now()

        try:
            # æ­¥éª¤1: æ›´æ–°Kçº¿æ•°æ®
            if 'step1' in skip_steps:
                self.logger.info("è·³è¿‡æ­¥éª¤1: æ›´æ–°Kçº¿æ•°æ®")
            else:
                if not self.step1_update_kline_data():
                    self.logger.error("æ­¥éª¤1å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                    return False

            # æ­¥éª¤1.5: æ›´æ–°CCIåº•èƒŒç¦»æ•°æ®
            if 'step1.5' in skip_steps or 'step1_5' in skip_steps:
                self.logger.info("è·³è¿‡æ­¥éª¤1.5: æ›´æ–°CCIåº•èƒŒç¦»æ•°æ®")
            else:
                if not self.step1_5_update_cci_divergence(custom_date=custom_date):
                    self.logger.error("æ­¥éª¤1.5å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                    return False

            # æ­¥éª¤2: ç”Ÿæˆä¹°å…¥ä¿¡å·
            if 'step2' in skip_steps:
                self.logger.info("è·³è¿‡æ­¥éª¤2: ç”Ÿæˆä¹°å…¥ä¿¡å·")
            else:
                if not self.step2_generate_signals(custom_date=custom_date):
                    self.logger.error("æ­¥éª¤2å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                    return False

            # æ­¥éª¤3: æ¨é€åˆ°å¾®ä¿¡
            if 'step3' in skip_steps:
                self.logger.info("è·³è¿‡æ­¥éª¤3: æ¨é€åˆ°å¾®ä¿¡")
            else:
                if not self.step3_push_to_wechat():
                    self.logger.error("æ­¥éª¤3å¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
                    return False

            # å…¨éƒ¨æˆåŠŸ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            self.logger.info("=" * 80)
            self.logger.info("æ‰§è¡Œå®Œæˆ")
            self.logger.info(f"æ€»è€—æ—¶: {duration:.2f}ç§’")
            self.logger.info("=" * 80)

            return True

        except Exception as e:
            self.logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            self.logger.error(traceback.format_exc())
            return False


def main():
    """ä¸»å‡½æ•° - æ”¯æŒrunå’Œqueryä¸¤ç§æ¨¡å¼"""
    # åˆ›å»ºä¸»è§£æå™¨
    parser = argparse.ArgumentParser(
        description='Daily Executor - è‡ªåŠ¨åŒ–è‚¡ç¥¨ä¿¡å·æ¨é€ç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # å…¨å±€å‚æ•°
    parser.add_argument(
        '--config', '-c',
        type=str,
        default='config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)'
    )

    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # ============================================================================
    # RUN å‘½ä»¤ - æ¯æ—¥è‡ªåŠ¨æ‰§è¡Œ
    # ============================================================================
    run_parser = subparsers.add_parser(
        'run',
        help='æ‰§è¡Œæ¯æ—¥è‡ªåŠ¨åŒ–æµç¨‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
  python daily_executor.py run

  # è·³è¿‡Kçº¿æ•°æ®æ›´æ–°
  python daily_executor.py run --skip-step1

  # æµ‹è¯•æ¨¡å¼ï¼ˆä¸æ¨é€ï¼‰
  python daily_executor.py run --dry-run

  # æŒ‡å®šå†å²æ—¥æœŸæµ‹è¯•
  python daily_executor.py run --date 2025-11-06 --skip-step1
        """
    )

    run_parser.add_argument(
        '--skip-step1',
        action='store_true',
        help='è·³è¿‡æ­¥éª¤1: æ›´æ–°Kçº¿æ•°æ®'
    )

    run_parser.add_argument(
        '--skip-step1.5',
        action='store_true',
        dest='skip_step1_5',
        help='è·³è¿‡æ­¥éª¤1.5: æ›´æ–°CCIåº•èƒŒç¦»æ•°æ®'
    )

    run_parser.add_argument(
        '--skip-step2',
        action='store_true',
        help='è·³è¿‡æ­¥éª¤2: ç”Ÿæˆä¹°å…¥ä¿¡å·'
    )

    run_parser.add_argument(
        '--skip-step3',
        action='store_true',
        help='è·³è¿‡æ­¥éª¤3: æ¨é€åˆ°å¾®ä¿¡'
    )

    run_parser.add_argument(
        '--dry-run',
        action='store_true',
        help='æ¼”ç»ƒæ¨¡å¼: è·³è¿‡æ¨é€æ­¥éª¤'
    )

    run_parser.add_argument(
        '--date', '-d',
        type=str,
        help='æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)'
    )

    # ============================================================================
    # QUERY å‘½ä»¤ - å†å²ä¿¡å·æŸ¥è¯¢
    # ============================================================================
    query_parser = subparsers.add_parser(
        'query',
        help='æŸ¥è¯¢å†å²ä¹°å…¥ä¿¡å·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æŸ¥è¯¢æŒ‡å®šæ—¥æœŸçš„ä¿¡å·
  python daily_executor.py query --date 2025-11-06

  # æŸ¥è¯¢æ—¥æœŸèŒƒå›´çš„ä¿¡å·
  python daily_executor.py query --date-range 2025-11-01 2025-11-10

  # æŸ¥è¯¢å¹¶è¾“å‡ºä¸ºJSON
  python daily_executor.py query --date 2025-11-06 --output json

  # æŸ¥è¯¢å¹¶æ¨é€åˆ°å¾®ä¿¡
  python daily_executor.py query --date 2025-11-06 --push-wechat

  # é«˜ç½®ä¿¡åº¦ä¿¡å·æŸ¥è¯¢
  python daily_executor.py query --date-range 2025-11-01 2025-11-10 --min-confidence 0.8

  # æŸ¥è¯¢ç‰¹å®šè‚¡ç¥¨
  python daily_executor.py query --date 2025-11-06 --stock-code 600519_SH
        """
    )

    # æ—¥æœŸå‚æ•°ï¼ˆäº’æ–¥ï¼‰
    date_group = query_parser.add_mutually_exclusive_group(required=True)
    date_group.add_argument(
        '--date', '-d',
        type=str,
        help='æŸ¥è¯¢å•ä¸ªæ—¥æœŸ (YYYY-MM-DD)'
    )

    date_group.add_argument(
        '--date-range',
        nargs=2,
        metavar=('START', 'END'),
        help='æŸ¥è¯¢æ—¥æœŸèŒƒå›´ (START END, YYYY-MM-DD)'
    )

    # è¿‡æ»¤å‚æ•°
    query_parser.add_argument(
        '--stock-code',
        type=str,
        help='æŒ‡å®šè‚¡ç¥¨ä»£ç  (å¦‚: 600519_SH)'
    )

    query_parser.add_argument(
        '--min-confidence',
        type=float,
        help='æœ€å°ç½®ä¿¡åº¦ (0.0-1.0)'
    )

    # è¾“å‡ºå‚æ•°
    query_parser.add_argument(
        '--output',
        action='append',
        choices=['console', 'csv', 'json'],
        help='è¾“å‡ºæ ¼å¼ (å¯å¤šæ¬¡æŒ‡å®š)'
    )

    query_parser.add_argument(
        '--push-wechat',
        action='store_true',
        help='æ¨é€æŸ¥è¯¢ç»“æœåˆ°å¾®ä¿¡'
    )

    query_parser.add_argument(
        '--output-file',
        type=str,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (ç”¨äºcsv/json)'
    )

    # è§£æå‚æ•°
    args = parser.parse_args()

    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‘½ä»¤ï¼Œé»˜è®¤ä½¿ç”¨runï¼ˆå‘åå…¼å®¹ï¼‰
    if args.command is None:
        args.command = 'run'
        # ä¿ç•™æ—§å‚æ•°çš„å‘åå…¼å®¹æ€§
        args.skip_step1 = False
        args.skip_step1_5 = False
        args.skip_step2 = False
        args.skip_step3 = False
        args.dry_run = False
        args.date = None

    try:
        if args.command == 'run':
            # RUNæ¨¡å¼ - æ‰§è¡Œæ¯æ—¥è‡ªåŠ¨åŒ–æµç¨‹
            skip_steps = []
            if args.skip_step1:
                skip_steps.append('step1')
            if args.skip_step1_5:
                skip_steps.append('step1.5')
            if args.skip_step2:
                skip_steps.append('step2')
            if args.skip_step3 or args.dry_run:
                skip_steps.append('step3')

            executor = DailyExecutor(config_file=args.config)
            success = executor.execute(skip_steps=skip_steps, custom_date=args.date)
            sys.exit(0 if success else 1)

        elif args.command == 'query':
            # QUERYæ¨¡å¼ - å†å²ä¿¡å·æŸ¥è¯¢ï¼ˆPhase 2å®ç°ï¼‰
            run_query_command(args)

    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        print(f"ç¨‹åºå¼‚å¸¸: {e}")
        traceback.print_exc()
        sys.exit(1)


def run_query_command(args):
    """
    æ‰§è¡Œqueryå‘½ä»¤ - å†å²ä¿¡å·æŸ¥è¯¢

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    import formatters

    print("=" * 80)
    print("å†å²ä¿¡å·æŸ¥è¯¢")
    print("=" * 80)

    try:
        # Load config
        config_path = Path(args.config)
        if not config_path.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            sys.exit(1)

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        signal_config = config['signal_generation']

        # Determine date range
        if args.date:
            start_date = args.date
            end_date = args.date
            query_desc = f"æ—¥æœŸ: {args.date}"
        elif args.date_range:
            start_date = args.date_range[0]
            end_date = args.date_range[1]
            query_desc = f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}"
        else:
            print("âŒ å¿…é¡»æŒ‡å®š --date æˆ– --date-range")
            sys.exit(1)

        # Parse stock codes filter
        stock_codes = None
        if args.stock_code:
            stock_codes = [args.stock_code]
            query_desc += f" | è‚¡ç¥¨: {args.stock_code}"

        # Parse confidence filter
        min_confidence = args.min_confidence if args.min_confidence else signal_config.get('min_confidence', 0.0)
        if args.min_confidence:
            query_desc += f" | æœ€å°ç½®ä¿¡åº¦: {min_confidence}"

        print(f"æŸ¥è¯¢æ¡ä»¶: {query_desc}")
        print()

        # Initialize QueryEngine
        query_engine = QueryEngine(
            db_path=signal_config['db_path'],
            data_dir=signal_config['data_dir']
        )

        # Execute query
        print("â³ æŸ¥è¯¢ä¸­...")
        signals = query_engine.fetch_signals(
            start_date=start_date,
            end_date=end_date,
            stock_codes=stock_codes,
            min_confidence=min_confidence,
            use_next_day_open=signal_config.get('use_next_day_open', True)
        )

        if not signals:
            print()
            print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¿¡å·")
            print()
            print("æç¤º:")
            print("  - æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦åœ¨æ•°æ®åº“èŒƒå›´å†…")
            print("  - å°è¯•é™ä½ --min-confidence é˜ˆå€¼")
            print("  - ç¡®è®¤CCIæ•°æ®åº“å·²åŒ…å«è¯¥æ—¶æœŸçš„æ•°æ®")
            print()
            sys.exit(0)

        print(f"âœ… æ‰¾åˆ° {len(signals)} ä¸ªä¿¡å·")
        print()

        # Determine output formats
        output_formats = args.output if args.output else ['console']

        # Process each output format
        for fmt in output_formats:
            if fmt == 'console':
                console_output = formatters.format_console(signals)
                print(console_output)
                print()

            elif fmt == 'csv':
                # Determine output file
                if args.output_file:
                    csv_file = args.output_file
                else:
                    # Auto-generate filename
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    csv_file = f"./signals/query_{start_date}_{timestamp}.csv"

                formatters.to_csv(signals, csv_file)
                print()

            elif fmt == 'json':
                # Determine output file
                if args.output_file:
                    json_file = args.output_file
                else:
                    # Auto-generate filename
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    json_file = f"./signals/query_{start_date}_{timestamp}.json"

                formatters.to_json(signals, json_file)
                print()

        # Push to WeChat if requested
        if args.push_wechat:
            print("ğŸ“± æ¨é€åˆ°å¾®ä¿¡...")
            try:
                # Create markdown message
                markdown_msg = formatters.to_wechat_markdown(
                    signals,
                    query_date=args.date if args.date else f"{start_date}~{end_date}"
                )

                # Use WechatPusher to send
                from wechat_pusher import WechatPusher

                pusher = WechatPusher(config_file=args.config)

                # Create a temporary message for push
                title = f"ğŸ“Š æŸ¥è¯¢ç»“æœ ({len(signals)}ä¸ªä¿¡å·)"

                # Push to all enabled recipients
                server_sauce_config = config['server_sauce']
                success_count = 0
                total_count = 0

                for recipient in server_sauce_config['recipients']:
                    if not recipient.get('enabled', True):
                        continue

                    total_count += 1

                    import requests
                    sendkey = recipient['sendkey']
                    url = f"https://sctapi.ftqq.com/{sendkey}.send"

                    response = requests.post(url, data={
                        'title': title,
                        'desp': markdown_msg
                    })

                    if response.status_code == 200:
                        result = response.json()
                        if result.get('code') == 0:
                            print(f"  âœ… æˆåŠŸæ¨é€åˆ°: {recipient['name']}")
                            success_count += 1
                        else:
                            print(f"  âŒ æ¨é€å¤±è´¥: {recipient['name']} - {result.get('message')}")
                    else:
                        print(f"  âŒ æ¨é€å¤±è´¥: {recipient['name']} - HTTP {response.status_code}")

                print()
                print(f"æ¨é€å®Œæˆ: {success_count}/{total_count} æˆåŠŸ")

            except Exception as e:
                print(f"âŒ æ¨é€å¤±è´¥: {e}")
                traceback.print_exc()

        print()
        print("=" * 80)
        print(f"æŸ¥è¯¢å®Œæˆ: {formatters.format_summary(signals)}")
        print("=" * 80)

        sys.exit(0)

    except Exception as e:
        print(f"\nâŒ æŸ¥è¯¢å¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
